{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radon-Transform (CUDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos, ceil\n",
    "import typing\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import radon\n",
    "import torch_radon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Backward transform is not transpose of forward",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/alexander/Projects/radon/tests.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alexander/Projects/radon/tests.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m Aa \u001b[39m=\u001b[39m radon\u001b[39m.\u001b[39mradon_forward(a, angles, positions)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alexander/Projects/radon/tests.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m Atb \u001b[39m=\u001b[39m radon\u001b[39m.\u001b[39mradon_backward(b, \u001b[39m32\u001b[39m, angles, positions)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alexander/Projects/radon/tests.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39mabs(torch\u001b[39m.\u001b[39msum(Aa\u001b[39m*\u001b[39mb)\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39msum(a\u001b[39m*\u001b[39mAtb)) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1e-1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBackward transform is not transpose of forward\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Backward transform is not transpose of forward"
     ]
    }
   ],
   "source": [
    "positions = torch.arange(-ceil(32*1.41421356237/2.0), ceil(32*1.41421356237/2.0)+1, dtype=torch.float32, device=\"cuda\")\n",
    "angles = torch.linspace(0.0, torch.pi, 33, device=\"cuda\")[:-1]\n",
    "a = torch.randn((1,1,32,32), device=\"cuda\")\n",
    "b = torch.randn((1,1,angles.shape[0],positions.shape[0]), device=\"cuda\")\n",
    "Aa = radon.radon_forward(a, angles, positions)\n",
    "Atb = radon.radon_backward(b, 32, angles, positions)\n",
    "assert torch.abs(torch.sum(Aa*b)-torch.sum(a*Atb)) <= 1e-1, \"Backward transform is not transpose of forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = torch.arange(-ceil(32*1.41421356237/2.0), ceil(32*1.41421356237/2.0)+1, dtype=torch.float32)\n",
    "angles = torch.linspace(0.0, torch.pi, 33)[:-1]\n",
    "torchradon = torch_radon.Radon(32, angles, positions.shape[0], 1)\n",
    "a = torch.randn((1,1,32,32), device=\"cuda\")\n",
    "b = torch.randn((1,1,angles.shape[0],positions.shape[0]), device=\"cuda\")\n",
    "Aa = torchradon.forward(a)\n",
    "Atb = torchradon.backprojection(b)\n",
    "print(torch.sum(Aa*b))\n",
    "print(torch.sum(a*Atb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.Caltech256(\"/data/datasets/\", download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Grayscale(), torchvision.transforms.Resize(512, antialias=True), torchvision.transforms.CenterCrop(512)]))\n",
    "rows, cols = 10, 7\n",
    "for i in range(10):\n",
    "    img = dataset[i][0].unsqueeze(0).contiguous().to(\"cuda\")\n",
    "    sino = radon.radon_forward(img)\n",
    "    fsino = radon.radon_filter(sino, radon.ram_lak_filter)\n",
    "    recon = radon.radon_backward(fsino, img.shape[-1])\n",
    "    assert torch.abs(recon.min()-img.min()) <= 1e-1, f\"Minimum not content invariant: {img.min()} vs {recon.min()}\"\n",
    "    assert torch.abs(recon.max()-img.max()) <= 1e-1, f\"Maximum not content invariant: {img.max()} vs {recon.max()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 10, 7\n",
    "for i in range(5):\n",
    "    size = [64,128,256,512,1024][i]\n",
    "    img = next(iter(torchvision.datasets.Caltech256(\"/data/datasets/\", download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Grayscale(), torchvision.transforms.Resize(size, antialias=True), torchvision.transforms.CenterCrop(size)]))))[0].unsqueeze(0).contiguous().to(\"cuda\")\n",
    "    sino = radon.radon_forward(img)\n",
    "    fsino = radon.radon_filter(sino, radon.ram_lak_filter)\n",
    "    recon = radon.radon_backward(fsino, img.shape[-1])\n",
    "    assert torch.abs(recon.min()-img.min()) <= 1e-1, f\"Minimum not resolution invariant: {img.min()} vs {recon.min()}\"\n",
    "    assert torch.abs(recon.max()-img.max()) <= 1e-1, f\"Maximum not resolution invariant: {img.max()} vs {recon.max()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(torchvision.datasets.Caltech256(\"/data/datasets/\", download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Grayscale(), torchvision.transforms.Resize(size, antialias=True), torchvision.transforms.CenterCrop(size)]))))[0].unsqueeze(0).contiguous().to(\"cuda\")\n",
    "rows, cols = 10, 7\n",
    "for i in range(7):\n",
    "    positions = [\n",
    "        torch.arange(-ceil(img.shape[2]*1.41421356237/2.0), ceil(img.shape[2]*1.41421356237/2.0)+1, device=\"cuda\", dtype=torch.float32), \n",
    "        torch.linspace(-ceil(img.shape[2]*1.41421356237/2.0), ceil(img.shape[2]*1.41421356237/2.0), 100, device=\"cuda\"), \n",
    "        torch.linspace(-ceil(img.shape[2]*1.41421356237/2.0), ceil(img.shape[2]*1.41421356237/2.0), 300, device=\"cuda\"), \n",
    "        torch.linspace(-ceil(img.shape[2]*1.41421356237/2.0), ceil(img.shape[2]*1.41421356237/2.0), 500, device=\"cuda\"),\n",
    "        torch.cumsum(torch.softmax(torch.randn((100,), device=\"cuda\"), 0), 0),\n",
    "        torch.cumsum(torch.softmax(torch.randn((300,), device=\"cuda\"), 0), 0),\n",
    "        torch.cumsum(torch.softmax(torch.randn((500,), device=\"cuda\"), 0), 0)\n",
    "    ][i]\n",
    "    sino = radon.radon_forward(img, positions=positions)\n",
    "    fsino = radon.radon_filter(sino, radon.ram_lak_filter)\n",
    "    recon = radon.radon_backward(fsino, img.shape[-1], positions=positions)\n",
    "    assert torch.abs(recon.min()-img.min()) <= 1e-1, f\"Minimum not position quantity invariant: {img.min()} vs {recon.min()}\"\n",
    "    assert torch.abs(recon.max()-img.max()) <= 1e-1, f\"Maximum not position quantity invariant: {img.max()} vs {recon.max()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(torchvision.datasets.Caltech256(\"/data/datasets/\", download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Grayscale(), torchvision.transforms.Resize(size, antialias=True), torchvision.transforms.CenterCrop(size)]))))[0].unsqueeze(0).contiguous().to(\"cuda\")\n",
    "rows, cols = 10, 7\n",
    "for i in range(6):\n",
    "    angles = [\n",
    "        torch.linspace(0.0, torch.pi, 100+1, device=\"cuda\")[:-1], \n",
    "        torch.linspace(0.0, torch.pi, 300+1, device=\"cuda\")[:-1], \n",
    "        torch.linspace(0.0, torch.pi, 500+1, device=\"cuda\")[:-1],\n",
    "        (torch.cumsum(torch.softmax(torch.randn((100+1,), device=\"cuda\"), 0), 0)*torch.pi)[:-1],\n",
    "        (torch.cumsum(torch.softmax(torch.randn((300+1,), device=\"cuda\"), 0), 0)*torch.pi)[:-1],\n",
    "        (torch.cumsum(torch.softmax(torch.randn((500+1,), device=\"cuda\"), 0), 0)*torch.pi)[:-1]\n",
    "    ][i]\n",
    "    sino = radon.radon_forward(img, positions=positions)\n",
    "    fsino = radon.radon_filter(sino, radon.ram_lak_filter)\n",
    "    recon = radon.radon_backward(fsino, img.shape[-1], positions=positions)\n",
    "    assert torch.abs(recon.min()-img.min()) <= 1e-1, f\"Minimum not angle quantity invariant: {img.min()} vs {recon.min()}\"\n",
    "    assert torch.abs(recon.max()-img.max()) <= 1e-1, f\"Maximum not angle quantity invariant: {img.max()} vs {recon.max()}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('FSDLIP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f5571690fd5745e3f997c7bba842cd44f89a0efbc12ee036449b5767e7cf64c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
